# #16 3Sum Closest

You are given a list of integers and a target. Find the sum of three integers in the list that would be the closest to the target and return it.

## Notes

At first glance, the question reminded me of a similar one I had done a month ago, this older question had you count the distinct k-difference pairs in a list. They were similar in that the sums/difference of multiple integers had to be or be as close to a certain target number. In that question, I had thought to make a seperate list with the elements subtracted from the number k. That way I could reiterate through the original list and look for each item one by one. Though in the end it was not to great of an idea since it took essentially O(n^2) runtime, but it was a good learning experience.

The main difference between this and that question was that I could not create another list of differences. The fact that the sum you computed was the sum of three integers would have made that solution take O(n!) as we had n!/6(n-3)! combinations of sums that would have to be relisted and searched. Therefore it would be faster to just calculate the sums on your own.

Another thought this had brought up to me was that these sum/difference archtype questions generally went one of two ways. Either you somehow speedily compare each possible sum/difference, or there was some underlying relationship the numbers had with each other. Usually if the elements were associated with each other in multiple dimensions, such that they had other variables that allowed us to compare them in another scale, then there was usually a solution that allowed us to easily eliminate unneccessary calculations. In this case, there was nothing to compare but the differences each number had from the target number.

I thought that the fastest way to do this was to have three pointers, with one pointer incrementing from the beginnining and the other two to represent the numbers after the first pointer. This solution worked sometimes, but on issue it had was that sometimes, although a number would get the sum closer to the target, the solution would actually contain the worse off number. And since there was no way of properly ordering the numbers so that continuing calculations would get the sum closer to the target, the algorithm would sometimes miss answers that were more spread out. To fix this I had to sort the list with the Python sort() function. After sorting, bringing the second and third pointer closer together made the sum closer for each unique number that pointer one referenced.

## Comments

I am writing this for my future self because I felt disappointed in the slow runtime I had somehow managed in doing this problem. Maybe other users had well tuned solutions or maybe I had an off day, but that doesn't change the fact that I feel as if I made a mistake. So from now on if I feel disatisfied with myself, I'll unload my thoughts into a Readme file for future reflections.

My first solution was based on a three pointer implementation. I start off by sorting the list, this would allow for more accurate searches as I will get into later. I would then have one pointer which would act as a 'pivot', where it would iterate from [0, list.length - 2], while the other two pointers would point at the index after pivot and the index list.length - 1. The latter two pointers, which we will call j and k, would collapse towards each other until they would equal each other, checking the sum as we went. The way I selected which pointer would move would be based on if the sum at the moment was lesser or greater than the target. If the sum was greater, then it would move k to the left, since the list was sorted in increasing orders, otherwise we would increase j. However, this would waste a lot of time on inputs that had early solutions or long lists since we had to iterate through the entire list.

The first solution beat a measly 14.79% of submissions in speed which sucks. It definitely didn't feel like a brute force solution as it optimized making comparisons. It did however calculate every possible sum before returning, so it most likely had a O(n^2) runtime. If this was an interview I think I'd have failed by now.

My second attempt was a step up from the first. Instead of tracking the sum at the moment, I thought it would be faster to track the difference from the target, that way instead of having to add the pivot element each time, we can just subtract the other two elements from a p_target, which is the difference between the pivot element and the target. On top of that, I added an early exit which would exit upon finding the correct sum. I believe most of this time was saved in adding the early exit, since it heavily favored lists that had solutions at the beginning.

The second solution did much better than the first, but still not quite there by only beating 39.86% of submissions.

In the third attempt, I had wondered if storing the differences was actually faster, and if the speed in the second attempt was only from the early exits. So this time I implemented the first solution again, except this time I allowed for early exits for when the sum was correct. Judging by the results, it was indeed faster to compare numbers as sums rather than differences.

The third solution made a significant dent in the average time spent on each test, winning out on 66.65% of submissions

Out of curiousity for how fast I could make my code, I tried again to whittle down the time. I looked for places where I thought time could be saved and I looked at an items variable which I had used to store the length of the list, although I had only referenced it twice I wondered if it would be faster to reference the length of a list as the return value of the __len__() function or as a reference. Suprisingly, referencing the length as __len__() was much faster, from being faster than 66.65% of submissions to 75.12%. Maybe it was the act of initializing the items variable, or maybe it was faster to make a function invocation over a reference. Either way I think it should normally not matter this much. If I access the length often, I'd probably use a reference variable, otherwise I'd call __len__() for one-time uses.

For a bit I continued optimizing the solution. The changes felt more like mirco-optimizations rather than some big brain algorithm development. By saving the mininum differences between the closest_sum and the target each time the closest_sum was updated, I made it so that instead of having to calculate the same sums twice in a row, the second calculation would instead be a reference. This small change brought the speed up to 86.06%. Its suprising how such a small change could make such a big difference. Makes you think though, how many micro-optimizations have the fastest submissions been through? This feels less like problem solving and more like a derby car race.

After fighting for what seemed to be inches of a football field, I began to lose interest in my little optimization game. I learned a good bit about micro-optimizations, but too bad it means little to nothing in the long run. Over the hour I took, I realized I should focus more on problem solving rather than being the fastest. 

I leave this here to remind myself, anyone can be fast with an hour of optimization, but the only thing that matters in the end is if my problem solving skills develop well enough to pass the whiteboard tests